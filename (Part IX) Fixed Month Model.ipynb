{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulag/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas\n",
    "import xarray\n",
    "import cftime\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:         (bounds: 2, month: 12, time: 7800)\n",
      "Coordinates:\n",
      "  * time            (time) float64 15.5 45.0 74.5 ... 2.372e+05 2.372e+05\n",
      "Dimensions without coordinates: bounds, month\n",
      "Data variables:\n",
      "    nino34          (time) float64 ...\n",
      "    time_bnds       (time, bounds) float64 ...\n",
      "    areacello       float32 ...\n",
      "    days_per_month  (month) int32 ...\n"
     ]
    }
   ],
   "source": [
    "datapath = 'nino34_monthly.nc'\n",
    "nino34 = xarray.open_dataset(datapath, decode_times = False)\n",
    "print(nino34)\n",
    "nino34 = np.array(nino34['nino34'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ONI(nino34, m = 3):\n",
    "    oni = np.array(nino34)\n",
    "    length = nino34.shape[0]\n",
    "    for i in range(length):\n",
    "        oni[i] = np.mean(nino34[max(0, (i - m + 1)) : min((i + 1), length)])\n",
    "    return oni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "oni = ONI(nino34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def climatology(nino34):\n",
    "    clm = np.zeros(12)\n",
    "    length = nino34.shape[0]\n",
    "    for month in range(12):\n",
    "        section = [12 * i + month for i in range(length // 12)]\n",
    "        clm[month] = np.mean(nino34[section])\n",
    "    return clm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clm = climatology(nino34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SST_anomaly(nino34, clm):\n",
    "    anm = np.array(nino34)\n",
    "    length = nino34.shape[0]\n",
    "    for i in range(length):\n",
    "        anm[i] = nino34[i] - clm[i % 12]\n",
    "    return anm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "anm = SST_anomaly(nino34, clm)\n",
    "oanm = ONI(anm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 6                       # prediction timeline\n",
    "H = 48                      # history used for prediction\n",
    "n_classes = 3               # number of classes (El Nino, El Nina, No Event)\n",
    "threshold = 0.5         \n",
    "signal = np.array(nino34[12 * 50:])   # data used for training/testing\n",
    "length = signal.shape[0]    # number of data points\n",
    "size = length - H - T       # effective dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the 'history matrix'\n",
    "data = np.ndarray((size, H))\n",
    "for i in range(size):\n",
    "    data[i] = signal[i:(i + H)]\n",
    "\n",
    "# label El Nino as 2, El Nina as 0 and no event as 1\n",
    "labels = np.ndarray((size))\n",
    "for i in range(length - H - T):\n",
    "    if(oanm[i + H + T] >= threshold):\n",
    "        labels[i] = 2\n",
    "    elif(oanm[i + H + T] <= -threshold):\n",
    "        labels[i] = 0\n",
    "    else:\n",
    "        labels[i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed Month (February)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "472/472 [==============================] - 3s 5ms/step - loss: 1.3238 - acc: 0.2691\n",
      "Epoch 2/40\n",
      "472/472 [==============================] - 0s 137us/step - loss: 1.0355 - acc: 0.4619\n",
      "Epoch 3/40\n",
      "472/472 [==============================] - 0s 178us/step - loss: 0.9667 - acc: 0.5805\n",
      "Epoch 4/40\n",
      "472/472 [==============================] - 0s 148us/step - loss: 0.9450 - acc: 0.6102\n",
      "Epoch 5/40\n",
      "472/472 [==============================] - 0s 121us/step - loss: 0.9344 - acc: 0.6059\n",
      "Epoch 6/40\n",
      "472/472 [==============================] - 0s 116us/step - loss: 0.9261 - acc: 0.6102\n",
      "Epoch 7/40\n",
      "472/472 [==============================] - 0s 103us/step - loss: 0.9195 - acc: 0.6059\n",
      "Epoch 8/40\n",
      "472/472 [==============================] - 0s 90us/step - loss: 0.9145 - acc: 0.6081\n",
      "Epoch 9/40\n",
      "472/472 [==============================] - 0s 85us/step - loss: 0.9082 - acc: 0.6081\n",
      "Epoch 10/40\n",
      "472/472 [==============================] - 0s 89us/step - loss: 0.9028 - acc: 0.6102\n",
      "Epoch 11/40\n",
      "472/472 [==============================] - 0s 91us/step - loss: 0.8991 - acc: 0.6102\n",
      "Epoch 12/40\n",
      "472/472 [==============================] - 0s 85us/step - loss: 0.8939 - acc: 0.6081\n",
      "Epoch 13/40\n",
      "472/472 [==============================] - 0s 91us/step - loss: 0.8897 - acc: 0.6081\n",
      "Epoch 14/40\n",
      "472/472 [==============================] - 0s 85us/step - loss: 0.8856 - acc: 0.6081\n",
      "Epoch 15/40\n",
      "472/472 [==============================] - 0s 94us/step - loss: 0.8826 - acc: 0.6102\n",
      "Epoch 16/40\n",
      "472/472 [==============================] - 0s 85us/step - loss: 0.8782 - acc: 0.6123\n",
      "Epoch 17/40\n",
      "472/472 [==============================] - 0s 97us/step - loss: 0.8747 - acc: 0.6102\n",
      "Epoch 18/40\n",
      "472/472 [==============================] - 0s 103us/step - loss: 0.8718 - acc: 0.6102\n",
      "Epoch 19/40\n",
      "472/472 [==============================] - 0s 107us/step - loss: 0.8677 - acc: 0.6123\n",
      "Epoch 20/40\n",
      "472/472 [==============================] - 0s 107us/step - loss: 0.8649 - acc: 0.6123\n",
      "Epoch 21/40\n",
      "472/472 [==============================] - 0s 108us/step - loss: 0.8620 - acc: 0.6123\n",
      "Epoch 22/40\n",
      "472/472 [==============================] - 0s 107us/step - loss: 0.8591 - acc: 0.6144\n",
      "Epoch 23/40\n",
      "472/472 [==============================] - 0s 153us/step - loss: 0.8559 - acc: 0.6144\n",
      "Epoch 24/40\n",
      "472/472 [==============================] - 0s 163us/step - loss: 0.8536 - acc: 0.6165\n",
      "Epoch 25/40\n",
      "472/472 [==============================] - 0s 158us/step - loss: 0.8500 - acc: 0.6165\n",
      "Epoch 26/40\n",
      "472/472 [==============================] - 0s 102us/step - loss: 0.8471 - acc: 0.6144\n",
      "Epoch 27/40\n",
      "472/472 [==============================] - 0s 93us/step - loss: 0.8441 - acc: 0.6144\n",
      "Epoch 28/40\n",
      "472/472 [==============================] - 0s 88us/step - loss: 0.8416 - acc: 0.6144\n",
      "Epoch 29/40\n",
      "472/472 [==============================] - 0s 93us/step - loss: 0.8392 - acc: 0.6144\n",
      "Epoch 30/40\n",
      "472/472 [==============================] - 0s 95us/step - loss: 0.8361 - acc: 0.6144\n",
      "Epoch 31/40\n",
      "472/472 [==============================] - 0s 88us/step - loss: 0.8333 - acc: 0.6186\n",
      "Epoch 32/40\n",
      "472/472 [==============================] - 0s 93us/step - loss: 0.8310 - acc: 0.6208\n",
      "Epoch 33/40\n",
      "472/472 [==============================] - 0s 94us/step - loss: 0.8286 - acc: 0.6208\n",
      "Epoch 34/40\n",
      "472/472 [==============================] - 0s 86us/step - loss: 0.8262 - acc: 0.6208\n",
      "Epoch 35/40\n",
      "472/472 [==============================] - 0s 96us/step - loss: 0.8233 - acc: 0.6208\n",
      "Epoch 36/40\n",
      "472/472 [==============================] - 0s 108us/step - loss: 0.8204 - acc: 0.6229\n",
      "Epoch 37/40\n",
      "472/472 [==============================] - 0s 167us/step - loss: 0.8189 - acc: 0.6314\n",
      "Epoch 38/40\n",
      "472/472 [==============================] - 0s 170us/step - loss: 0.8158 - acc: 0.6229\n",
      "Epoch 39/40\n",
      "472/472 [==============================] - 0s 136us/step - loss: 0.8132 - acc: 0.6335\n",
      "Epoch 40/40\n",
      "472/472 [==============================] - 0s 108us/step - loss: 0.8106 - acc: 0.6292\n",
      "59/59 [==============================] - 1s 17ms/step\n",
      "0.6101694935459202\n"
     ]
    }
   ],
   "source": [
    "# predicting february\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "size2 = size // 12\n",
    "    \n",
    "split = size2 // 10      \n",
    "shuffle = 12 * np.random.permutation(size2) + np.ones(size2, dtype = int)\n",
    "    \n",
    "train_ind = np.array(shuffle[0: 8 * split])\n",
    "val_ind = np.array(shuffle[8 * split: 9 * split])\n",
    "test_ind = np.array(shuffle[9 * split: size2])\n",
    "\n",
    "train = np.array(data[train_ind])\n",
    "train_labels = np.array(labels[train_ind])\n",
    "\n",
    "val = np.array(data[val_ind])\n",
    "val_labels = np.array(labels[val_ind])\n",
    "\n",
    "test = np.array(data[test_ind])\n",
    "test_labels = np.array(labels[test_ind])\n",
    "    \n",
    "mean = np.mean(train, axis = 0)\n",
    "std = np.std(train, axis = 0)\n",
    "\n",
    "train_n = np.divide(\n",
    "        train - np.outer(np.ones(train.shape[0]), mean),\n",
    "        np.outer(np.ones(train.shape[0]), std))\n",
    "\n",
    "val_n = np.divide(\n",
    "        val - np.outer(np.ones(val.shape[0]), mean),\n",
    "        np.outer(np.ones(val.shape[0]), std))\n",
    "\n",
    "test_n = np.divide(\n",
    "        test - np.outer(np.ones(test.shape[0]), mean),\n",
    "        np.outer(np.ones(test.shape[0]), std))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        encoded_train_labels = tf.one_hot(train_labels, depth = n_classes).eval()\n",
    "        encoded_val_labels = tf.one_hot(val_labels, depth = n_classes).eval()\n",
    "        encoded_test_labels = tf.one_hot(test_labels, depth = n_classes).eval()\n",
    "        \n",
    "(d, N1, N2) = (0.5, 128, 64)\n",
    "epochs = 40\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "              tf.keras.layers.Dense(N1, activation=tf.nn.relu),\n",
    "              tf.keras.layers.Dropout(d),\n",
    "              tf.keras.layers.Dense(N2, activation=tf.nn.relu),\n",
    "              tf.keras.layers.Dropout(d),\n",
    "              tf.keras.layers.Dense(n_classes, activation=tf.nn.softmax)\n",
    "            ])\n",
    "\n",
    "model.reset_states()\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_n, encoded_train_labels, epochs = epochs)\n",
    "(loss_m, acc_m) = model.evaluate(val_n, encoded_val_labels)\n",
    "\n",
    "print(acc_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.34 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn import multiclass\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = multiclass.OneVsRestClassifier(SVC(kernel='rbf', random_state = 0))\n",
    "clf.fit(train_n, train_labels)\n",
    "acc_SVC = clf.score(val_n, val_labels)\n",
    "\n",
    "print(np.around(acc_SVC * 100, decimals = 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed Season (Winter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1424/1424 [==============================] - 3s 2ms/step - loss: 1.0091 - acc: 0.5506\n",
      "Epoch 2/30\n",
      "1424/1424 [==============================] - 0s 78us/step - loss: 0.9614 - acc: 0.6053\n",
      "Epoch 3/30\n",
      "1424/1424 [==============================] - 0s 77us/step - loss: 0.9495 - acc: 0.6060\n",
      "Epoch 4/30\n",
      "1424/1424 [==============================] - 0s 82us/step - loss: 0.9411 - acc: 0.6074\n",
      "Epoch 5/30\n",
      "1424/1424 [==============================] - 0s 76us/step - loss: 0.9341 - acc: 0.6060\n",
      "Epoch 6/30\n",
      "1424/1424 [==============================] - 0s 82us/step - loss: 0.9290 - acc: 0.6081\n",
      "Epoch 7/30\n",
      "1424/1424 [==============================] - 0s 80us/step - loss: 0.9226 - acc: 0.6067\n",
      "Epoch 8/30\n",
      "1424/1424 [==============================] - 0s 80us/step - loss: 0.9199 - acc: 0.6074\n",
      "Epoch 9/30\n",
      "1424/1424 [==============================] - 0s 81us/step - loss: 0.9161 - acc: 0.6074\n",
      "Epoch 10/30\n",
      "1424/1424 [==============================] - 0s 84us/step - loss: 0.9131 - acc: 0.6081\n",
      "Epoch 11/30\n",
      "1424/1424 [==============================] - 0s 84us/step - loss: 0.9098 - acc: 0.6074\n",
      "Epoch 12/30\n",
      "1424/1424 [==============================] - 0s 83us/step - loss: 0.9072 - acc: 0.6067\n",
      "Epoch 13/30\n",
      "1424/1424 [==============================] - 0s 79us/step - loss: 0.9044 - acc: 0.6096\n",
      "Epoch 14/30\n",
      "1424/1424 [==============================] - 0s 85us/step - loss: 0.9016 - acc: 0.6074\n",
      "Epoch 15/30\n",
      "1424/1424 [==============================] - 0s 88us/step - loss: 0.9001 - acc: 0.6096\n",
      "Epoch 16/30\n",
      "1424/1424 [==============================] - 0s 80us/step - loss: 0.8976 - acc: 0.6088\n",
      "Epoch 17/30\n",
      "1424/1424 [==============================] - 0s 80us/step - loss: 0.8958 - acc: 0.6081\n",
      "Epoch 18/30\n",
      "1424/1424 [==============================] - 0s 94us/step - loss: 0.8937 - acc: 0.6096\n",
      "Epoch 19/30\n",
      "1424/1424 [==============================] - 0s 89us/step - loss: 0.8920 - acc: 0.6088\n",
      "Epoch 20/30\n",
      "1424/1424 [==============================] - 0s 90us/step - loss: 0.8901 - acc: 0.6103\n",
      "Epoch 21/30\n",
      "1424/1424 [==============================] - 0s 91us/step - loss: 0.8883 - acc: 0.6096\n",
      "Epoch 22/30\n",
      "1424/1424 [==============================] - 0s 83us/step - loss: 0.8866 - acc: 0.6096\n",
      "Epoch 23/30\n",
      "1424/1424 [==============================] - 0s 135us/step - loss: 0.8851 - acc: 0.6103\n",
      "Epoch 24/30\n",
      "1424/1424 [==============================] - 0s 148us/step - loss: 0.8835 - acc: 0.6096\n",
      "Epoch 25/30\n",
      "1424/1424 [==============================] - 0s 118us/step - loss: 0.8820 - acc: 0.6096\n",
      "Epoch 26/30\n",
      "1424/1424 [==============================] - 0s 124us/step - loss: 0.8804 - acc: 0.6103\n",
      "Epoch 27/30\n",
      "1424/1424 [==============================] - 0s 83us/step - loss: 0.8791 - acc: 0.6088\n",
      "Epoch 28/30\n",
      "1424/1424 [==============================] - 0s 78us/step - loss: 0.8776 - acc: 0.6124\n",
      "Epoch 29/30\n",
      "1424/1424 [==============================] - 0s 80us/step - loss: 0.8764 - acc: 0.6124\n",
      "Epoch 30/30\n",
      "1424/1424 [==============================] - 0s 83us/step - loss: 0.8754 - acc: 0.6131\n",
      "178/178 [==============================] - 1s 7ms/step\n",
      "0.5280898869707343\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "size2 = size // 12\n",
    "    \n",
    "split = (3 * size2) // 10    \n",
    "shuffle = np.zeros(size2 * 3, dtype = int)\n",
    "shuffle[0:size2] = 12 * np.random.permutation(size2)\n",
    "shuffle[size2: 2 * size2] = 12 * np.random.permutation(size2) + 1 * np.ones(size2, dtype = int) #feb\n",
    "shuffle[2 * size2: 3 * size2] = 12 * np.random.permutation(size2) + 11 * np.ones(size2, dtype = int) #dec\n",
    "    \n",
    "train_ind = np.array(shuffle[0: 8 * split])\n",
    "val_ind = np.array(shuffle[8 * split: 9 * split])\n",
    "test_ind = np.array(shuffle[9 * split: size2])\n",
    "\n",
    "train = np.array(data[train_ind])\n",
    "train_labels = np.array(labels[train_ind])\n",
    "\n",
    "val = np.array(data[val_ind])\n",
    "val_labels = np.array(labels[val_ind])\n",
    "\n",
    "test = np.array(data[test_ind])\n",
    "test_labels = np.array(labels[test_ind])\n",
    "    \n",
    "mean = np.mean(train, axis = 0)\n",
    "std = np.std(train, axis = 0)\n",
    "\n",
    "train_n = np.divide(\n",
    "        train - np.outer(np.ones(train.shape[0]), mean),\n",
    "        np.outer(np.ones(train.shape[0]), std))\n",
    "\n",
    "val_n = np.divide(\n",
    "        val - np.outer(np.ones(val.shape[0]), mean),\n",
    "        np.outer(np.ones(val.shape[0]), std))\n",
    "\n",
    "test_n = np.divide(\n",
    "        test - np.outer(np.ones(test.shape[0]), mean),\n",
    "        np.outer(np.ones(test.shape[0]), std))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        encoded_train_labels = tf.one_hot(train_labels, depth = n_classes).eval()\n",
    "        encoded_val_labels = tf.one_hot(val_labels, depth = n_classes).eval()\n",
    "        encoded_test_labels = tf.one_hot(test_labels, depth = n_classes).eval()\n",
    "        \n",
    "(d, N1, N2) = (0.5, 128, 64)\n",
    "epochs = 30\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "              tf.keras.layers.Dense(N1, activation=tf.nn.relu),\n",
    "              tf.keras.layers.Dropout(d),\n",
    "              tf.keras.layers.Dense(N2, activation=tf.nn.relu),\n",
    "              tf.keras.layers.Dropout(d),\n",
    "              tf.keras.layers.Dense(n_classes, activation=tf.nn.softmax)\n",
    "            ])\n",
    "\n",
    "model.reset_states()\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(lr=0.01, momentum=0.001, decay=0.001, nesterov=False)\n",
    "\n",
    "model.compile(optimizer=sgd,\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_n, encoded_train_labels, epochs = epochs)\n",
    "(loss_m, acc_m) = model.evaluate(val_n, encoded_val_labels)\n",
    "\n",
    "print(acc_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.81 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn import multiclass\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = multiclass.OneVsRestClassifier(SVC(kernel='rbf', random_state = 0))\n",
    "clf.fit(train_n, train_labels)\n",
    "acc_SVC = clf.score(val_n, val_labels)\n",
    "\n",
    "print(np.around(acc_SVC * 100, decimals = 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed Season (Spring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "1424/1424 [==============================] - 5s 4ms/step - loss: 1.0576 - acc: 0.5000\n",
      "Epoch 2/35\n",
      "1424/1424 [==============================] - 1s 538us/step - loss: 1.0043 - acc: 0.5232\n",
      "Epoch 3/35\n",
      "1424/1424 [==============================] - 1s 828us/step - loss: 0.9766 - acc: 0.5253\n",
      "Epoch 4/35\n",
      "1424/1424 [==============================] - 1s 904us/step - loss: 0.9350 - acc: 0.5597\n",
      "Epoch 5/35\n",
      "1424/1424 [==============================] - 1s 861us/step - loss: 0.9049 - acc: 0.5787\n",
      "Epoch 6/35\n",
      "1424/1424 [==============================] - 1s 927us/step - loss: 0.8357 - acc: 0.6131\n",
      "Epoch 7/35\n",
      "1424/1424 [==============================] - 1s 630us/step - loss: 0.7648 - acc: 0.6559\n",
      "Epoch 8/35\n",
      "1424/1424 [==============================] - 1s 739us/step - loss: 0.7183 - acc: 0.6840\n",
      "Epoch 9/35\n",
      "1424/1424 [==============================] - 1s 649us/step - loss: 0.6204 - acc: 0.7360\n",
      "Epoch 10/35\n",
      "1424/1424 [==============================] - 1s 657us/step - loss: 0.5617 - acc: 0.7584\n",
      "Epoch 11/35\n",
      "1424/1424 [==============================] - 1s 586us/step - loss: 0.4367 - acc: 0.8357\n",
      "Epoch 12/35\n",
      "1424/1424 [==============================] - 1s 626us/step - loss: 0.3713 - acc: 0.8574\n",
      "Epoch 13/35\n",
      "1424/1424 [==============================] - 1s 646us/step - loss: 0.3357 - acc: 0.8631\n",
      "Epoch 14/35\n",
      "1424/1424 [==============================] - 1s 593us/step - loss: 0.2724 - acc: 0.8919\n",
      "Epoch 15/35\n",
      "1424/1424 [==============================] - 1s 613us/step - loss: 0.2427 - acc: 0.9221\n",
      "Epoch 16/35\n",
      "1424/1424 [==============================] - 1s 592us/step - loss: 0.1610 - acc: 0.9515\n",
      "Epoch 17/35\n",
      "1424/1424 [==============================] - 1s 553us/step - loss: 0.1545 - acc: 0.9494\n",
      "Epoch 18/35\n",
      "1424/1424 [==============================] - 1s 634us/step - loss: 0.1121 - acc: 0.9649\n",
      "Epoch 19/35\n",
      "1424/1424 [==============================] - 1s 601us/step - loss: 0.1033 - acc: 0.9656\n",
      "Epoch 20/35\n",
      "1424/1424 [==============================] - 1s 612us/step - loss: 0.0830 - acc: 0.9782\n",
      "Epoch 21/35\n",
      "1424/1424 [==============================] - 1s 606us/step - loss: 0.0915 - acc: 0.9705\n",
      "Epoch 22/35\n",
      "1424/1424 [==============================] - 1s 596us/step - loss: 0.0510 - acc: 0.9881\n",
      "Epoch 23/35\n",
      "1424/1424 [==============================] - 1s 537us/step - loss: 0.0551 - acc: 0.9874\n",
      "Epoch 24/35\n",
      "1424/1424 [==============================] - 1s 464us/step - loss: 0.0515 - acc: 0.9881\n",
      "Epoch 25/35\n",
      "1424/1424 [==============================] - 1s 570us/step - loss: 0.0388 - acc: 0.9853\n",
      "Epoch 26/35\n",
      "1424/1424 [==============================] - 1s 539us/step - loss: 0.0897 - acc: 0.9733\n",
      "Epoch 27/35\n",
      "1424/1424 [==============================] - 1s 617us/step - loss: 0.1031 - acc: 0.9705\n",
      "Epoch 28/35\n",
      "1424/1424 [==============================] - 1s 541us/step - loss: 0.0859 - acc: 0.9754\n",
      "Epoch 29/35\n",
      "1424/1424 [==============================] - 1s 519us/step - loss: 0.0516 - acc: 0.9860\n",
      "Epoch 30/35\n",
      "1424/1424 [==============================] - 1s 499us/step - loss: 0.1323 - acc: 0.9670\n",
      "Epoch 31/35\n",
      "1424/1424 [==============================] - 1s 585us/step - loss: 0.1446 - acc: 0.9565\n",
      "Epoch 32/35\n",
      "1424/1424 [==============================] - 1s 700us/step - loss: 0.0544 - acc: 0.9831\n",
      "Epoch 33/35\n",
      " 224/1424 [===>..........................] - ETA: 0s - loss: 0.0266 - acc: 0.9955"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "size2 = size // 12\n",
    "    \n",
    "split = (3 * size2) // 10    \n",
    "shuffle = np.zeros(size2 * 3, dtype = int)\n",
    "shuffle[0:size2] = 12 * np.random.permutation(size2) + 2 * np.ones(size2, dtype = int) # march\n",
    "shuffle[size2: 2 * size2] = 12 * np.random.permutation(size2) + 3 * np.ones(size2, dtype = int) # april\n",
    "shuffle[2 * size2: 3 * size2] = 12 * np.random.permutation(size2) + 4 * np.ones(size2, dtype = int) # may\n",
    "    \n",
    "train_ind = np.array(shuffle[0: 8 * split])\n",
    "val_ind = np.array(shuffle[8 * split: 9 * split])\n",
    "test_ind = np.array(shuffle[9 * split: size2])\n",
    "\n",
    "train = np.array(data[train_ind])\n",
    "train_labels = np.array(labels[train_ind])\n",
    "\n",
    "val = np.array(data[val_ind])\n",
    "val_labels = np.array(labels[val_ind])\n",
    "\n",
    "test = np.array(data[test_ind])\n",
    "test_labels = np.array(labels[test_ind])\n",
    "    \n",
    "mean = np.mean(train, axis = 0)\n",
    "std = np.std(train, axis = 0)\n",
    "\n",
    "train_n = np.divide(\n",
    "        train - np.outer(np.ones(train.shape[0]), mean),\n",
    "        np.outer(np.ones(train.shape[0]), std))\n",
    "\n",
    "val_n = np.divide(\n",
    "        val - np.outer(np.ones(val.shape[0]), mean),\n",
    "        np.outer(np.ones(val.shape[0]), std))\n",
    "\n",
    "test_n = np.divide(\n",
    "        test - np.outer(np.ones(test.shape[0]), mean),\n",
    "        np.outer(np.ones(test.shape[0]), std))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        encoded_train_labels = tf.one_hot(train_labels, depth = n_classes).eval()\n",
    "        encoded_val_labels = tf.one_hot(val_labels, depth = n_classes).eval()\n",
    "        encoded_test_labels = tf.one_hot(test_labels, depth = n_classes).eval()\n",
    "        \n",
    "(d, N1, N2, N3) = (0.3, 768, 768, 384)\n",
    "epochs = 35\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "              tf.keras.layers.Dense(N1, activation=tf.nn.relu),\n",
    "              tf.keras.layers.Dropout(d),\n",
    "              tf.keras.layers.Dense(N2, activation=tf.nn.relu),\n",
    "              tf.keras.layers.Dropout(d),\n",
    "              tf.keras.layers.Dense(N3, activation=tf.nn.relu),\n",
    "              tf.keras.layers.Dropout(d),\n",
    "              tf.keras.layers.Dense(n_classes, activation=tf.nn.softmax)\n",
    "            ])\n",
    "\n",
    "model.reset_states()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_n, encoded_train_labels, epochs = epochs)\n",
    "(loss_m, acc_m) = model.evaluate(val_n, encoded_val_labels)\n",
    "\n",
    "print(acc_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.57 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn import multiclass\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = multiclass.OneVsRestClassifier(SVC(kernel='rbf', random_state = 0))\n",
    "clf.fit(train_n, train_labels)\n",
    "acc_SVC = clf.score(val_n, val_labels)\n",
    "\n",
    "print(np.around(acc_SVC * 100, decimals = 2), \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
