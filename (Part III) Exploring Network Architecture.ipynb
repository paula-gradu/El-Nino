{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulag/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas\n",
    "import xarray\n",
    "import cftime\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:         (bounds: 2, month: 12, time: 7800)\n",
      "Coordinates:\n",
      "  * time            (time) float64 15.5 45.0 74.5 ... 2.372e+05 2.372e+05\n",
      "Dimensions without coordinates: bounds, month\n",
      "Data variables:\n",
      "    nino34          (time) float64 ...\n",
      "    time_bnds       (time, bounds) float64 ...\n",
      "    areacello       float32 ...\n",
      "    days_per_month  (month) int32 ...\n"
     ]
    }
   ],
   "source": [
    "datapath = 'nino34_monthly.nc'\n",
    "nino34 = xarray.open_dataset(datapath, decode_times = False)\n",
    "print(nino34)\n",
    "nino34 = np.array(nino34['nino34'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ONI(nino34, m = 3):\n",
    "    oni = np.array(nino34)\n",
    "    length = nino34.shape[0]\n",
    "    for i in range(length):\n",
    "        oni[i] = np.mean(nino34[max(0, (i - m + 1)) : min((i + 1), length)])\n",
    "    return oni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "oni = ONI(nino34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def climatology(nino34):\n",
    "    clm = np.zeros(12)\n",
    "    length = nino34.shape[0]\n",
    "    for month in range(12):\n",
    "        section = [12 * i + month for i in range(length // 12)]\n",
    "        clm[month] = np.mean(nino34[section])\n",
    "    return clm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clm = climatology(nino34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SST_anomaly(nino34, clm):\n",
    "    anm = np.array(nino34)\n",
    "    length = nino34.shape[0]\n",
    "    for i in range(length):\n",
    "        anm[i] = nino34[i] - clm[i % 12]\n",
    "    return anm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "anm = SST_anomaly(nino34, clm)\n",
    "oanm = ONI(anm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 6                       # prediction timeline\n",
    "H = 48                      # history used for prediction\n",
    "include_month = 0           # 1 if we use the month as a feature, 0 otherwise\n",
    "n_classes = 3               # number of classes (El Nino, El Nina, No Event)\n",
    "threshold = 0.5         \n",
    "signal = np.array(nino34)   # data used for training/testing\n",
    "length = signal.shape[0]    # number of data points\n",
    "size = length - H - T       # effective dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the 'history matrix'\n",
    "data = np.ndarray((size, H + include_month))\n",
    "for i in range(size):\n",
    "    if(include_month == False):\n",
    "        data[i] = signal[i:(i + H)]\n",
    "    else:\n",
    "        data[i] = np.append(signal[i:(i + H)], (i + H + T) % 12)\n",
    "\n",
    "# label El Nino as 2, El Nina as 0 and no event as 1\n",
    "labels = np.ndarray((size))\n",
    "for i in range(length - H - T):\n",
    "    if(oanm[i + H + T] >= threshold):\n",
    "        labels[i] = 2\n",
    "    elif(oanm[i + H + T] <= -threshold):\n",
    "        labels[i] = 0\n",
    "    else:\n",
    "        labels[i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into 80% training, 10% validation and 10% testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "split = size // 10      \n",
    "shuffle = np.random.permutation(size)\n",
    "train_ind = np.array(shuffle[0: 8 * split])\n",
    "val_ind = np.array(shuffle[(8 * split + 1): 9 * split])\n",
    "test_ind = np.array(shuffle[(9 * split + 1): size])\n",
    "\n",
    "train = np.array(data[train_ind])\n",
    "train_labels = np.array(labels[train_ind])\n",
    "\n",
    "val = np.array(data[val_ind])\n",
    "val_labels = np.array(labels[val_ind])\n",
    "\n",
    "test = np.array(data[test_ind])\n",
    "test_labels = np.array(labels[test_ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization and Label One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(train)\n",
    "std = np.std(train)\n",
    "train_n = (train - mean) / std\n",
    "val_n = (val - mean) / std\n",
    "test_n = (test - mean) / std\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    encoded_train_labels = tf.one_hot(train_labels, depth = n_classes).eval()\n",
    "    encoded_val_labels = tf.one_hot(val_labels, depth = n_classes).eval()\n",
    "    encoded_test_labels = tf.one_hot(test_labels, depth = n_classes).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A One Hidden Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we try to get an idea about what the simplest neural network architecture can achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  epochs,  48  nodes,  0.0  dropout 0.5549805952383043 % accuracy\n",
      "1  epochs,  48  nodes,  0.2  dropout 0.548512289934294 % accuracy\n",
      "1  epochs,  48  nodes,  0.5  dropout 0.5446313066362255 % accuracy\n",
      "1  epochs,  96  nodes,  0.0  dropout 0.5692108667914648 % accuracy\n",
      "1  epochs,  96  nodes,  0.2  dropout 0.5782664942170793 % accuracy\n",
      "1  epochs,  96  nodes,  0.5  dropout 0.5536869341775024 % accuracy\n",
      "1  epochs,  192  nodes,  0.0  dropout 0.5705045279679293 % accuracy\n",
      "1  epochs,  192  nodes,  0.2  dropout 0.5679172057692169 % accuracy\n",
      "1  epochs,  192  nodes,  0.5  dropout 0.5730918499738711 % accuracy\n",
      "1  epochs,  288  nodes,  0.0  dropout 0.5523932731167003 % accuracy\n",
      "1  epochs,  288  nodes,  0.2  dropout 0.5782664943327417 % accuracy\n",
      "1  epochs,  288  nodes,  0.5  dropout 0.5834411384602876 % accuracy\n",
      "5  epochs,  48  nodes,  0.0  dropout 0.5485122898186318 % accuracy\n",
      "5  epochs,  48  nodes,  0.2  dropout 0.5730918499738711 % accuracy\n",
      "5  epochs,  48  nodes,  0.5  dropout 0.5601552393658503 % accuracy\n",
      "5  epochs,  96  nodes,  0.0  dropout 0.584734799636752 % accuracy\n",
      "5  epochs,  96  nodes,  0.2  dropout 0.5808538164543458 % accuracy\n",
      "5  epochs,  96  nodes,  0.5  dropout 0.5873221217583562 % accuracy\n",
      "5  epochs,  192  nodes,  0.0  dropout 0.5743855110346732 % accuracy\n",
      "5  epochs,  192  nodes,  0.2  dropout 0.5795601552778814 % accuracy\n",
      "5  epochs,  192  nodes,  0.5  dropout 0.5976714101291104 % accuracy\n",
      "5  epochs,  288  nodes,  0.0  dropout 0.566623544785523 % accuracy\n",
      "5  epochs,  288  nodes,  0.2  dropout 0.5821474773994856 % accuracy\n",
      "5  epochs,  288  nodes,  0.5  dropout 0.5666235446698608 % accuracy\n",
      "10  epochs,  48  nodes,  0.0  dropout 0.5808538164543458 % accuracy\n",
      "10  epochs,  48  nodes,  0.2  dropout 0.5730918499738711 % accuracy\n",
      "10  epochs,  48  nodes,  0.5  dropout 0.6015523934271789 % accuracy\n",
      "10  epochs,  96  nodes,  0.0  dropout 0.589909443764298 % accuracy\n",
      "10  epochs,  96  nodes,  0.2  dropout 0.6041397154331207 % accuracy\n",
      "10  epochs,  96  nodes,  0.5  dropout 0.6002587323663768 % accuracy\n",
      "10  epochs,  192  nodes,  0.0  dropout 0.589909443764298 % accuracy\n",
      "10  epochs,  192  nodes,  0.2  dropout 0.5782664943327417 % accuracy\n",
      "10  epochs,  192  nodes,  0.5  dropout 0.5963777490683083 % accuracy\n",
      "10  epochs,  288  nodes,  0.0  dropout 0.589909443764298 % accuracy\n",
      "10  epochs,  288  nodes,  0.2  dropout 0.5963777490683083 % accuracy\n",
      "10  epochs,  288  nodes,  0.5  dropout 0.5860284605818917 % accuracy\n",
      "20  epochs,  48  nodes,  0.0  dropout 0.5976714101291104 % accuracy\n",
      "20  epochs,  48  nodes,  0.2  dropout 0.5989650711899125 % accuracy\n",
      "20  epochs,  48  nodes,  0.5  dropout 0.5912031049407623 % accuracy\n",
      "20  epochs,  96  nodes,  0.0  dropout 0.5989650711899125 % accuracy\n",
      "20  epochs,  96  nodes,  0.2  dropout 0.5873221216426938 % accuracy\n",
      "20  epochs,  96  nodes,  0.5  dropout 0.6041397154331207 % accuracy\n",
      "20  epochs,  192  nodes,  0.0  dropout 0.5937904270623665 % accuracy\n",
      "20  epochs,  192  nodes,  0.2  dropout 0.5847347995210896 % accuracy\n",
      "20  epochs,  192  nodes,  0.5  dropout 0.5769728331562772 % accuracy\n",
      "20  epochs,  288  nodes,  0.0  dropout 0.6093143597919913 % accuracy\n",
      "20  epochs,  288  nodes,  0.2  dropout 0.6015523933115166 % accuracy\n",
      "20  epochs,  288  nodes,  0.5  dropout 0.5692108669071272 % accuracy\n"
     ]
    }
   ],
   "source": [
    "for E in [1, 5, 10, 20]:\n",
    "    for i in [1, 2, 4, 6]:\n",
    "        for d in [0.0, 0.2, 0.5]:\n",
    "        \n",
    "            N = i * H\n",
    "        \n",
    "            model = tf.keras.models.Sequential([\n",
    "              tf.keras.layers.Dense(N, activation=tf.nn.relu),\n",
    "              tf.keras.layers.Dropout(d),\n",
    "              tf.keras.layers.Dense(n_classes, activation=tf.nn.softmax)\n",
    "            ])\n",
    "            \n",
    "            model.reset_states()\n",
    "\n",
    "            model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "            model.fit(train_n, encoded_train_labels, epochs = E, verbose = 0)\n",
    "            (loss, acc) = model.evaluate(val_n, encoded_val_labels, verbose = 0)\n",
    "            print(E, \" epochs, \", N, \" nodes, \", d, \" dropout\", acc, \"% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above are consistent with the intuition that more epochs and more nodes produce better results. They also showcase that 0.5 dropout might be too much. Although from the results above we may think that dropout is not that useful, it usually is the case that deeper networks benefit more from the addition of dropout layers and therefore we will not discount the technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Deep Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fix the number of epochs to 25 and vary dropout, the number of layers and the number of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "loss = {}\n",
    "acc = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2  layers,  192  nodes,  0.2  dropout 0.645536869378787 % accuracy\n",
      "2  layers,  192  nodes,  0.3  dropout 0.6222509702843497 % accuracy\n",
      "2  layers,  192  nodes,  0.5  dropout 0.6481241916160534 % accuracy\n",
      "2  layers,  384  nodes,  0.2  dropout 0.676584734953699 % accuracy\n",
      "2  layers,  384  nodes,  0.3  dropout 0.6778783960145011 % accuracy\n",
      "2  layers,  384  nodes,  0.5  dropout 0.6804657181361051 % accuracy\n",
      "2  layers,  768  nodes,  0.2  dropout 0.6545924968044015 % accuracy\n",
      "2  layers,  768  nodes,  0.3  dropout 0.7128072446561569 % accuracy\n",
      "2  layers,  768  nodes,  0.5  dropout 0.684346701202849 % accuracy\n",
      "3  layers,  192  nodes,  0.2  dropout 0.6895213455617196 % accuracy\n",
      "3  layers,  192  nodes,  0.3  dropout 0.66364812426857 % accuracy\n",
      "3  layers,  192  nodes,  0.5  dropout 0.6662354463516201 % accuracy\n",
      "3  layers,  384  nodes,  0.2  dropout 0.714100905716959 % accuracy\n",
      "3  layers,  384  nodes,  0.3  dropout 0.7192755499601673 % accuracy\n",
      "3  layers,  384  nodes,  0.5  dropout 0.6972833118494238 % accuracy\n",
      "3  layers,  768  nodes,  0.2  dropout 0.6934023286284635 % accuracy\n",
      "3  layers,  768  nodes,  0.3  dropout 0.6895213455617196 % accuracy\n",
      "3  layers,  768  nodes,  0.5  dropout 0.7283311773086736 % accuracy\n",
      "4  layers,  192  nodes,  0.2  dropout 0.6895213454846114 % accuracy\n",
      "4  layers,  192  nodes,  0.3  dropout 0.6701164295725804 % accuracy\n",
      "4  layers,  192  nodes,  0.5  dropout 0.6714100905948284 % accuracy\n",
      "4  layers,  384  nodes,  0.2  dropout 0.7037516171148801 % accuracy\n",
      "4  layers,  384  nodes,  0.3  dropout 0.7037516172305425 % accuracy\n",
      "4  layers,  384  nodes,  0.5  dropout 0.6765847348380366 % accuracy\n",
      "4  layers,  768  nodes,  0.2  dropout 0.6675291074509763 % accuracy\n",
      "4  layers,  768  nodes,  0.3  dropout 0.699870633932474 % accuracy\n",
      "4  layers,  768  nodes,  0.5  dropout 0.7089262613580883 % accuracy\n"
     ]
    }
   ],
   "source": [
    "for l in [2, 3, 4]:\n",
    "    for i in [4, 8, 16]:\n",
    "        for d in [0.2, 0.3, 0.5]:\n",
    "            \n",
    "            N = i * H\n",
    "            \n",
    "            layers = []\n",
    "            \n",
    "            for n in range(l): \n",
    "                layers.append(tf.keras.layers.Dense(N, activation=tf.nn.relu))\n",
    "                layers.append(tf.keras.layers.Dropout(d))\n",
    "            \n",
    "            layers.append(tf.keras.layers.Dense(n_classes, activation=tf.nn.softmax))\n",
    "            \n",
    "            model = tf.keras.models.Sequential(layers)\n",
    "\n",
    "            model.reset_states()\n",
    "\n",
    "            model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "            model.fit(train_n, encoded_train_labels, epochs = epochs, verbose = 0)\n",
    "            (loss[(l, i, d)], acc[(l, i, d)]) = model.evaluate(val_n, encoded_val_labels, verbose = 0)\n",
    "            print(l, \" layers, \", N, \" nodes, \", d, \" dropout\", acc[(l, i, d)], \"% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing whether more epochs are necessary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will pick the top 5 best architectures from above and check if any of them would benefit from more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5 = [(2, 768, 0.3), (3, 384, 0.2), (3, 384, 0.3), (3, 768, 0.5), (4, 768, 0.5)]\n",
    "acc_e = {}\n",
    "loss_e = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 epochs 2  layers,  768  nodes,  0.3  dropout 0.6714100907104906 % accuracy\n",
      "30 epochs 3  layers,  384  nodes,  0.2  dropout 0.7347994826126839 % accuracy\n",
      "30 epochs 3  layers,  384  nodes,  0.3  dropout 0.7128072445790486 % accuracy\n",
      "30 epochs 3  layers,  768  nodes,  0.5  dropout 0.7089262614737507 % accuracy\n",
      "30 epochs 4  layers,  768  nodes,  0.5  dropout 0.7115135835953549 % accuracy\n",
      "40 epochs 2  layers,  768  nodes,  0.3  dropout 0.7102199225345528 % accuracy\n",
      "40 epochs 3  layers,  384  nodes,  0.2  dropout 0.7153945666620988 % accuracy\n",
      "40 epochs 3  layers,  384  nodes,  0.3  dropout 0.73350582162899 % accuracy\n",
      "40 epochs 3  layers,  768  nodes,  0.5  dropout 0.6908150065454135 % accuracy\n",
      "40 epochs 4  layers,  768  nodes,  0.5  dropout 0.6998706340481362 % accuracy\n",
      "50 epochs 2  layers,  768  nodes,  0.3  dropout 0.7322121605681879 % accuracy\n",
      "50 epochs 3  layers,  384  nodes,  0.2  dropout 0.7542043986018231 % accuracy\n",
      "50 epochs 3  layers,  384  nodes,  0.3  dropout 0.7063389391979301 % accuracy\n",
      "50 epochs 3  layers,  768  nodes,  0.5  dropout 0.7412677879938024 % accuracy\n",
      "50 epochs 4  layers,  768  nodes,  0.5  dropout 0.7244501942033755 % accuracy\n"
     ]
    }
   ],
   "source": [
    "for e in [30, 40, 50]:\n",
    "    for m in top5:\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        (l, N, d) = m\n",
    "        \n",
    "        for n in range(l): \n",
    "            layers.append(tf.keras.layers.Dense(N, activation=tf.nn.relu))\n",
    "            layers.append(tf.keras.layers.Dropout(d))\n",
    "            \n",
    "        layers.append(tf.keras.layers.Dense(n_classes, activation=tf.nn.softmax))\n",
    "            \n",
    "        model = tf.keras.models.Sequential(layers)\n",
    "\n",
    "        model.reset_states()\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "        model.fit(train_n, encoded_train_labels, epochs = e, verbose = 0)\n",
    "        (loss_e[(e, l, i, d)], acc_e[(e, l, i, d)]) = model.evaluate(val_n, encoded_val_labels, verbose = 0)\n",
    "        print(e, \"epochs\", l, \" layers, \", N, \" nodes, \", d, \" dropout\", acc_e[(e, l, i, d)], \"% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varying individual layer widths "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results above, it makes sense to stick to a 3 hidden layer architecture and train it for 50 epochs. Sometimes it helps if the network has layers of different widths. We will test if this is the case in our problem below. We will still vary dropout because it is still unclear whether we should go for a conservative rate of ~0.2 or for the harsher 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "loss_w = {}\n",
    "acc_w = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i1 in [4, 8, 16, 32]:\n",
    "    for i2 in [4, 8, 16, 32]:\n",
    "        for i3 in [4, 8, 16, 32]:\n",
    "            for d in [0.2, 0.3, 0.5]:\n",
    "            \n",
    "                model = tf.keras.models.Sequential([\n",
    "                  tf.keras.layers.Dense(i1 * H, activation=tf.nn.relu),\n",
    "                  tf.keras.layers.Dropout(d),\n",
    "                  tf.keras.layers.Dense(i2 * H, activation=tf.nn.relu),\n",
    "                  tf.keras.layers.Dropout(d),\n",
    "                  tf.keras.layers.Dense(i3 * H, activation=tf.nn.relu),\n",
    "                  tf.keras.layers.Dropout(d),\n",
    "                  tf.keras.layers.Dense(n_classes, activation=tf.nn.softmax)\n",
    "                ])\n",
    "\n",
    "                model.reset_states()\n",
    "\n",
    "                model.compile(optimizer='adam',\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "                model.fit(train_n, encoded_train_labels, epochs = epochs, verbose = 0)\n",
    "                (loss_w[(d, i1, i2, i3)], acc_w[(d, i1, i2, i3)]) = model.evaluate(val_n, encoded_val_labels, verbose = 0)\n",
    "                print(\"dropout:\", d, \"layers of widths\", i1 * H, i2 * H, i3 * H, \"=>\", acc_w[(d, i1, i2, i3)], \"% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect which architectures did better than 74.5% accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_models = {k:v for (k,v) in acc_w.items() if v > 0.745}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = [(0.3, 192, 768, 1536), (0.2, 384, 768, 384), (0.5, 384, 768, 384), (0.3, 384, 768, 768), \n",
    "               (0.5, 768, 384, 768), (0.2, 768, 384, 1536), (0.5, 768, 384, 1536), (0.3, 768, 768, 384),\n",
    "               (0.2, 768, 1536, 768), (0.5, 768, 1536, 768), (0.2, 1536, 384, 384), (0.5, 1536, 768, 192),\n",
    "               (0.3, 1536, 768, 384), (0.3, 1536, 768, 768), (0.3, 1536, 768, 1536), (0.5, 1536, 1536, 384)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now explore how Batch Normalization impacts the best models above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_b = {}\n",
    "acc_b = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout 0.3 layers of widths: 192 768 1536 0.7283311773086736\n",
      "Dropout 0.2 layers of widths: 384 768 384 0.7335058214747736\n",
      "Dropout 0.5 layers of widths: 384 768 384 0.7438551100382984\n",
      "Dropout 0.3 layers of widths: 384 768 768 0.7063389392364843\n",
      "Dropout 0.5 layers of widths: 768 384 768 0.7309184993531694\n",
      "Dropout 0.2 layers of widths: 768 384 1536 0.7153945667006528\n",
      "Dropout 0.5 layers of widths: 768 384 1536 0.7296248383116445\n",
      "Dropout 0.3 layers of widths: 768 768 384 0.7619663649666356\n",
      "Dropout 0.2 layers of widths: 768 1536 768 0.7477360932207046\n",
      "Dropout 0.5 layers of widths: 768 1536 768 0.7399741268558921\n",
      "Dropout 0.2 layers of widths: 1536 384 384 0.7296248383309215\n",
      "Dropout 0.5 layers of widths: 1536 768 192 0.7451487711762087\n",
      "Dropout 0.3 layers of widths: 1536 768 384 0.7425614490546045\n",
      "Dropout 0.3 layers of widths: 1536 768 768 0.7529107374639129\n",
      "Dropout 0.3 layers of widths: 1536 768 1536 0.73350582162899\n",
      "Dropout 0.5 layers of widths: 1536 1536 384 0.7438551100382984\n"
     ]
    }
   ],
   "source": [
    "for m in best_models:\n",
    "    \n",
    "    (d, N1, N2, N3) = m\n",
    "    model = tf.keras.models.Sequential([\n",
    "                  tf.keras.layers.Dense(N1, activation=tf.nn.relu),\n",
    "                  tf.keras.layers.Dropout(d),\n",
    "                  tf.keras.layers.BatchNormalization(),\n",
    "                  tf.keras.layers.Dense(N2, activation=tf.nn.relu),\n",
    "                  tf.keras.layers.Dropout(d),\n",
    "                  tf.keras.layers.BatchNormalization(),\n",
    "                  tf.keras.layers.Dense(N3, activation=tf.nn.relu),\n",
    "                  tf.keras.layers.Dropout(d),\n",
    "                  tf.keras.layers.BatchNormalization(),\n",
    "                  tf.keras.layers.Dense(n_classes, activation=tf.nn.softmax)\n",
    "                ])\n",
    "\n",
    "    model.reset_states()\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_n, encoded_train_labels, epochs = epochs, verbose = 0)\n",
    "    (loss_b[(d, i1, i2, i3)], acc_b[(d, i1, i2, i3)]) = model.evaluate(val_n, encoded_val_labels, verbose = 0)\n",
    "    print(\"Dropout\", d, \"layers of widths:\", N1, N2, N3, acc_b[(d, i1, i2, i3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Batch Normalization provides no notable benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring various activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_f = np.zeros(6)\n",
    "loss_f = np.zeros(6)\n",
    "function_name = [\"sigmoid\", \"tanh\", \"relu\", \"leaky relu\", \"selu\"]\n",
    "t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu_01(x):\n",
    "    return tf.nn.leaky_relu(x, alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid => 0.5912031048251001 % accuracy\n",
      "tanh => 0.6144890039195373 % accuracy\n",
      "relu => 0.7192755498830591 % accuracy\n",
      "leaky relu => 0.7322121605681879 % accuracy\n",
      "selu => 0.6015523933115166 % accuracy\n",
      "sigmoid => 0.6157826650188934 % accuracy\n",
      "tanh => 0.7244501940877133 % accuracy\n",
      "relu => 0.7141009056398507 % accuracy\n",
      "leaky relu => 0.7115135835182467 % accuracy\n",
      "selu => 0.6041397154331207 % accuracy\n",
      "sigmoid => 0.6222509702843497 % accuracy\n",
      "tanh => 0.6856403623793135 % accuracy\n",
      "relu => 0.7166882278385631 % accuracy\n",
      "leaky relu => 0.7115135834796925 % accuracy\n",
      "selu => 0.589909443802852 % accuracy\n",
      "sigmoid => 0.6248382925216162 % accuracy\n",
      "tanh => 0.6830530402577093 % accuracy\n",
      "relu => 0.7347994825355757 % accuracy\n",
      "leaky relu => 0.736093143673486 % accuracy\n",
      "selu => 0.571798188913069 % accuracy\n",
      "sigmoid => 0.6002587323663768 % accuracy\n",
      "tanh => 0.6571798189260056 % accuracy\n",
      "relu => 0.7399741269330004 % accuracy\n",
      "leaky relu => 0.7360931436349318 % accuracy\n",
      "selu => 0.5705045278522669 % accuracy\n",
      "sigmoid => 0.609314359676329 % accuracy\n",
      "tanh => 0.5808538163386835 % accuracy\n",
      "relu => 0.73350582162899 % accuracy\n",
      "leaky relu => 0.7244501940877133 % accuracy\n",
      "selu => 0.5821474775151478 % accuracy\n",
      "sigmoid => 0.6183699871019435 % accuracy\n",
      "tanh => 0.6041397154331207 % accuracy\n",
      "relu => 0.7050452782142362 % accuracy\n",
      "leaky relu => 0.7257438551099612 % accuracy\n",
      "selu => 0.5692108667914648 % accuracy\n",
      "sigmoid => 0.6183699871404976 % accuracy\n",
      "tanh => 0.7231565331425734 % accuracy\n",
      "relu => 0.7503234154194169 % accuracy\n",
      "leaky relu => 0.7179818888993652 % accuracy\n",
      "selu => 0.5912031048251001 % accuracy\n",
      "sigmoid => 0.5950840880075062 % accuracy\n",
      "tanh => 0.7011642949932759 % accuracy\n",
      "relu => 0.7503234153423087 % accuracy\n",
      "leaky relu => 0.7373868047342881 % accuracy\n",
      "selu => 0.5717981890287313 % accuracy\n",
      "sigmoid => 0.610608020737131 % accuracy\n",
      "tanh => 0.7283311773086736 % accuracy\n",
      "relu => 0.717981888822257 % accuracy\n",
      "leaky relu => 0.7477360932978128 % accuracy\n",
      "selu => 0.5976714101291104 % accuracy\n",
      "sigmoid => 0.6235446313451518 % accuracy\n",
      "tanh => 0.6856403623793135 % accuracy\n",
      "relu => 0.7231565331425734 % accuracy\n",
      "leaky relu => 0.7542043985247149 % accuracy\n",
      "selu => 0.6287192757040224 % accuracy\n",
      "sigmoid => 0.6041397154331207 % accuracy\n",
      "tanh => 0.681759379081245 % accuracy\n",
      "relu => 0.7283311772701194 % accuracy\n",
      "leaky relu => 0.7166882278385631 % accuracy\n",
      "selu => 0.6287192757040224 % accuracy\n",
      "sigmoid => 0.6002587322507145 % accuracy\n",
      "tanh => 0.7296248384465838 % accuracy\n",
      "relu => 0.7347994826126839 % accuracy\n",
      "leaky relu => 0.7296248383116445 % accuracy\n",
      "selu => 0.592496765885902 % accuracy\n",
      "sigmoid => 0.6015523933115166 % accuracy\n",
      "tanh => 0.7373868047342881 % accuracy\n",
      "relu => 0.7645536870882397 % accuracy\n",
      "leaky relu => 0.7373868047342881 % accuracy\n",
      "selu => 0.6067270376703872 % accuracy\n",
      "sigmoid => 0.5653298836090587 % accuracy\n",
      "tanh => 0.7063389393521465 % accuracy\n",
      "relu => 0.7296248384465838 % accuracy\n",
      "leaky relu => 0.7115135834411385 % accuracy\n",
      "selu => 0.5640362225482566 % accuracy\n",
      "sigmoid => 0.6119016817979331 % accuracy\n",
      "tanh => 0.6662354464672824 % accuracy\n",
      "relu => 0.7231565331425734 % accuracy\n",
      "leaky relu => 0.7270375162478715 % accuracy\n",
      "selu => 0.5834411385759499 % accuracy\n"
     ]
    }
   ],
   "source": [
    "for m in best_models:\n",
    "    (d, N1, N2, N3) = m\n",
    "    t = 0\n",
    "    for f in [tf.nn.sigmoid, tf.nn.tanh, tf.nn.relu, leaky_relu_01, tf.nn.selu]:\n",
    "    \n",
    "        model = tf.keras.models.Sequential([\n",
    "              tf.keras.layers.Dense(N1, activation=f),\n",
    "              tf.keras.layers.Dropout(d),\n",
    "              tf.keras.layers.Dense(N2, activation=f),\n",
    "              tf.keras.layers.Dropout(d),\n",
    "              tf.keras.layers.Dense(N3, activation=f),\n",
    "              tf.keras.layers.Dropout(d),\n",
    "              tf.keras.layers.Dense(n_classes, activation=tf.nn.softmax)\n",
    "            ])\n",
    "\n",
    "        model.reset_states()\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "          loss='categorical_crossentropy',\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "        model.fit(train_n, encoded_train_labels, epochs = epochs, verbose = 0)\n",
    "        (loss_f[t], acc_f[t]) = model.evaluate(val_n, encoded_val_labels, verbose = 0)\n",
    "        print(function_name[t], \"=>\", acc_f[t], \"% accuracy\")\n",
    "        t+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the best three are tanh, relu and leaky relu. Tanh seems to be more volatile, while the differences between leaky relu and relu are minor. It makes sense therefore to stick with the most popular option (also our initial option): relu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
