{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulag/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas\n",
    "import xarray\n",
    "import cftime\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:         (bounds: 2, month: 12, time: 7800)\n",
      "Coordinates:\n",
      "  * time            (time) float64 15.5 45.0 74.5 ... 2.372e+05 2.372e+05\n",
      "Dimensions without coordinates: bounds, month\n",
      "Data variables:\n",
      "    nino34          (time) float64 ...\n",
      "    time_bnds       (time, bounds) float64 ...\n",
      "    areacello       float32 ...\n",
      "    days_per_month  (month) int32 ...\n"
     ]
    }
   ],
   "source": [
    "datapath = 'nino34_monthly.nc'\n",
    "nino34 = xarray.open_dataset(datapath, decode_times = False)\n",
    "print(nino34)\n",
    "nino34 = np.array(nino34['nino34'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ONI(nino34, m = 3):\n",
    "    oni = np.array(nino34)\n",
    "    length = nino34.shape[0]\n",
    "    for i in range(length):\n",
    "        oni[i] = np.mean(nino34[max(0, (i - m + 1)) : min((i + 1), length)])\n",
    "    return oni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "oni = ONI(nino34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def climatology(nino34):\n",
    "    clm = np.zeros(12)\n",
    "    length = nino34.shape[0]\n",
    "    for month in range(12):\n",
    "        section = [12 * i + month for i in range(length // 12)]\n",
    "        clm[month] = np.mean(nino34[section])\n",
    "    return clm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clm = climatology(nino34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SST_anomaly(nino34, clm):\n",
    "    anm = np.array(nino34)\n",
    "    length = nino34.shape[0]\n",
    "    for i in range(length):\n",
    "        anm[i] = nino34[i] - clm[i % 12]\n",
    "    return anm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "anm = SST_anomaly(nino34, clm)\n",
    "oanm = ONI(anm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 6                       # prediction timeline\n",
    "H = 48                      # history used for prediction\n",
    "include_month = 0           # 1 if we use the month as a feature, 0 otherwise\n",
    "n_classes = 3               # number of classes (El Nino, El Nina, No Event)\n",
    "threshold = 0.5         \n",
    "signal = np.array(nino34)   # data used for training/testing\n",
    "length = signal.shape[0]    # number of data points\n",
    "size = length - H - T       # effective dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the 'history matrix'\n",
    "data = np.ndarray((size, H + include_month))\n",
    "for i in range(size):\n",
    "    if(include_month == False):\n",
    "        data[i] = signal[i:(i + H)]\n",
    "    else:\n",
    "        data[i] = np.append(signal[i:(i + H)], (i + H + T) % 12)\n",
    "\n",
    "# label El Nino as 2, El Nina as 0 and no event as 1\n",
    "labels = np.ndarray((size))\n",
    "for i in range(length - H - T):\n",
    "    if(oanm[i + H + T] >= threshold):\n",
    "        labels[i] = 2\n",
    "    elif(oanm[i + H + T] <= -threshold):\n",
    "        labels[i] = 0\n",
    "    else:\n",
    "        labels[i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into 80% training, 10% validation and 10% testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "split = size // 10      \n",
    "shuffle = np.random.permutation(size)\n",
    "train_ind = np.array(shuffle[0: 8 * split])\n",
    "val_ind = np.array(shuffle[(8 * split + 1): 9 * split])\n",
    "test_ind = np.array(shuffle[(9 * split + 1): size])\n",
    "\n",
    "train = np.array(data[train_ind])\n",
    "train_labels = np.array(labels[train_ind])\n",
    "\n",
    "val = np.array(data[val_ind])\n",
    "val_labels = np.array(labels[val_ind])\n",
    "\n",
    "test = np.array(data[test_ind])\n",
    "test_labels = np.array(labels[test_ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization and Label One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(train)\n",
    "std = np.std(train)\n",
    "train_n = (train - mean) / std\n",
    "val_n = (val - mean) / std\n",
    "test_n = (test - mean) / std\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    encoded_train_labels = tf.one_hot(train_labels, depth = n_classes).eval()\n",
    "    encoded_val_labels = tf.one_hot(val_labels, depth = n_classes).eval()\n",
    "    encoded_test_labels = tf.one_hot(test_labels, depth = n_classes).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A One Hidden Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we try to get an idea about what the simplest neural network architecture can achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  epochs,  48  nodes,  0.0  dropout 0.5549805952383043 % accuracy\n",
      "1  epochs,  48  nodes,  0.2  dropout 0.548512289934294 % accuracy\n",
      "1  epochs,  48  nodes,  0.5  dropout 0.5446313066362255 % accuracy\n",
      "1  epochs,  96  nodes,  0.0  dropout 0.5692108667914648 % accuracy\n",
      "1  epochs,  96  nodes,  0.2  dropout 0.5782664942170793 % accuracy\n",
      "1  epochs,  96  nodes,  0.5  dropout 0.5536869341775024 % accuracy\n",
      "1  epochs,  192  nodes,  0.0  dropout 0.5705045279679293 % accuracy\n",
      "1  epochs,  192  nodes,  0.2  dropout 0.5679172057692169 % accuracy\n",
      "1  epochs,  192  nodes,  0.5  dropout 0.5730918499738711 % accuracy\n",
      "1  epochs,  288  nodes,  0.0  dropout 0.5523932731167003 % accuracy\n",
      "1  epochs,  288  nodes,  0.2  dropout 0.5782664943327417 % accuracy\n",
      "1  epochs,  288  nodes,  0.5  dropout 0.5834411384602876 % accuracy\n",
      "5  epochs,  48  nodes,  0.0  dropout 0.5485122898186318 % accuracy\n",
      "5  epochs,  48  nodes,  0.2  dropout 0.5730918499738711 % accuracy\n",
      "5  epochs,  48  nodes,  0.5  dropout 0.5601552393658503 % accuracy\n",
      "5  epochs,  96  nodes,  0.0  dropout 0.584734799636752 % accuracy\n",
      "5  epochs,  96  nodes,  0.2  dropout 0.5808538164543458 % accuracy\n",
      "5  epochs,  96  nodes,  0.5  dropout 0.5873221217583562 % accuracy\n",
      "5  epochs,  192  nodes,  0.0  dropout 0.5743855110346732 % accuracy\n",
      "5  epochs,  192  nodes,  0.2  dropout 0.5795601552778814 % accuracy\n",
      "5  epochs,  192  nodes,  0.5  dropout 0.5976714101291104 % accuracy\n",
      "5  epochs,  288  nodes,  0.0  dropout 0.566623544785523 % accuracy\n",
      "5  epochs,  288  nodes,  0.2  dropout 0.5821474773994856 % accuracy\n",
      "5  epochs,  288  nodes,  0.5  dropout 0.5666235446698608 % accuracy\n",
      "10  epochs,  48  nodes,  0.0  dropout 0.5808538164543458 % accuracy\n",
      "10  epochs,  48  nodes,  0.2  dropout 0.5730918499738711 % accuracy\n",
      "10  epochs,  48  nodes,  0.5  dropout 0.6015523934271789 % accuracy\n",
      "10  epochs,  96  nodes,  0.0  dropout 0.589909443764298 % accuracy\n",
      "10  epochs,  96  nodes,  0.2  dropout 0.6041397154331207 % accuracy\n",
      "10  epochs,  96  nodes,  0.5  dropout 0.6002587323663768 % accuracy\n",
      "10  epochs,  192  nodes,  0.0  dropout 0.589909443764298 % accuracy\n",
      "10  epochs,  192  nodes,  0.2  dropout 0.5782664943327417 % accuracy\n",
      "10  epochs,  192  nodes,  0.5  dropout 0.5963777490683083 % accuracy\n",
      "10  epochs,  288  nodes,  0.0  dropout 0.589909443764298 % accuracy\n",
      "10  epochs,  288  nodes,  0.2  dropout 0.5963777490683083 % accuracy\n",
      "10  epochs,  288  nodes,  0.5  dropout 0.5860284605818917 % accuracy\n",
      "20  epochs,  48  nodes,  0.0  dropout 0.5976714101291104 % accuracy\n",
      "20  epochs,  48  nodes,  0.2  dropout 0.5989650711899125 % accuracy\n",
      "20  epochs,  48  nodes,  0.5  dropout 0.5912031049407623 % accuracy\n",
      "20  epochs,  96  nodes,  0.0  dropout 0.5989650711899125 % accuracy\n",
      "20  epochs,  96  nodes,  0.2  dropout 0.5873221216426938 % accuracy\n",
      "20  epochs,  96  nodes,  0.5  dropout 0.6041397154331207 % accuracy\n",
      "20  epochs,  192  nodes,  0.0  dropout 0.5937904270623665 % accuracy\n",
      "20  epochs,  192  nodes,  0.2  dropout 0.5847347995210896 % accuracy\n",
      "20  epochs,  192  nodes,  0.5  dropout 0.5769728331562772 % accuracy\n",
      "20  epochs,  288  nodes,  0.0  dropout 0.6093143597919913 % accuracy\n",
      "20  epochs,  288  nodes,  0.2  dropout 0.6015523933115166 % accuracy\n",
      "20  epochs,  288  nodes,  0.5  dropout 0.5692108669071272 % accuracy\n"
     ]
    }
   ],
   "source": [
    "for E in [1, 5, 10, 20]:\n",
    "    for i in [1, 2, 4, 6]:\n",
    "        for d in [0.0, 0.2, 0.5]:\n",
    "        \n",
    "            N = i * H\n",
    "        \n",
    "            model = tf.keras.models.Sequential([\n",
    "              tf.keras.layers.Dense(N, activation=tf.nn.relu),\n",
    "              tf.keras.layers.Dropout(d),\n",
    "              tf.keras.layers.Dense(n_classes, activation=tf.nn.softmax)\n",
    "            ])\n",
    "            \n",
    "            model.reset_states()\n",
    "\n",
    "            model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "            model.fit(train_n, encoded_train_labels, epochs = E, verbose = 0)\n",
    "            (loss, acc) = model.evaluate(val_n, encoded_val_labels, verbose = 0)\n",
    "            print(E, \" epochs, \", N, \" nodes, \", d, \" dropout\", acc, \"% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above are consistent with the intuition that more epochs and more nodes produce better results. They also showcase that 0.5 dropout might be too much. Although from the results above we may think that dropout is not that useful, it usually is the case that deeper networks benefit more from the addition of dropout layers and therefore we will not discount the technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Deep Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fix the number of epochs to 25 and vary dropout, the number of layers and the number of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "loss = {}\n",
    "acc = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2  layers,  192  nodes,  0.2  dropout 0.645536869378787 % accuracy\n",
      "2  layers,  192  nodes,  0.3  dropout 0.6222509702843497 % accuracy\n",
      "2  layers,  192  nodes,  0.5  dropout 0.6481241916160534 % accuracy\n",
      "2  layers,  384  nodes,  0.2  dropout 0.676584734953699 % accuracy\n",
      "2  layers,  384  nodes,  0.3  dropout 0.6778783960145011 % accuracy\n",
      "2  layers,  384  nodes,  0.5  dropout 0.6804657181361051 % accuracy\n",
      "2  layers,  768  nodes,  0.2  dropout 0.6545924968044015 % accuracy\n",
      "2  layers,  768  nodes,  0.3  dropout 0.7128072446561569 % accuracy\n",
      "2  layers,  768  nodes,  0.5  dropout 0.684346701202849 % accuracy\n",
      "3  layers,  192  nodes,  0.2  dropout 0.6895213455617196 % accuracy\n",
      "3  layers,  192  nodes,  0.3  dropout 0.66364812426857 % accuracy\n",
      "3  layers,  192  nodes,  0.5  dropout 0.6662354463516201 % accuracy\n",
      "3  layers,  384  nodes,  0.2  dropout 0.714100905716959 % accuracy\n",
      "3  layers,  384  nodes,  0.3  dropout 0.7192755499601673 % accuracy\n",
      "3  layers,  384  nodes,  0.5  dropout 0.6972833118494238 % accuracy\n",
      "3  layers,  768  nodes,  0.2  dropout 0.6934023286284635 % accuracy\n",
      "3  layers,  768  nodes,  0.3  dropout 0.6895213455617196 % accuracy\n",
      "3  layers,  768  nodes,  0.5  dropout 0.7283311773086736 % accuracy\n",
      "4  layers,  192  nodes,  0.2  dropout 0.6895213454846114 % accuracy\n",
      "4  layers,  192  nodes,  0.3  dropout 0.6701164295725804 % accuracy\n",
      "4  layers,  192  nodes,  0.5  dropout 0.6714100905948284 % accuracy\n",
      "4  layers,  384  nodes,  0.2  dropout 0.7037516171148801 % accuracy\n",
      "4  layers,  384  nodes,  0.3  dropout 0.7037516172305425 % accuracy\n",
      "4  layers,  384  nodes,  0.5  dropout 0.6765847348380366 % accuracy\n",
      "4  layers,  768  nodes,  0.2  dropout 0.6675291074509763 % accuracy\n",
      "4  layers,  768  nodes,  0.3  dropout 0.699870633932474 % accuracy\n",
      "4  layers,  768  nodes,  0.5  dropout 0.7089262613580883 % accuracy\n"
     ]
    }
   ],
   "source": [
    "for l in [2, 3, 4]:\n",
    "    for i in [4, 8, 16]:\n",
    "        for d in [0.2, 0.3, 0.5]:\n",
    "            \n",
    "            N = i * H\n",
    "            \n",
    "            layers = []\n",
    "            \n",
    "            for n in range(l): \n",
    "                layers.append(tf.keras.layers.Dense(N, activation=tf.nn.relu))\n",
    "                layers.append(tf.keras.layers.Dropout(d))\n",
    "            \n",
    "            layers.append(tf.keras.layers.Dense(n_classes, activation=tf.nn.softmax))\n",
    "            \n",
    "            model = tf.keras.models.Sequential(layers)\n",
    "\n",
    "            model.reset_states()\n",
    "\n",
    "            model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "            model.fit(train_n, encoded_train_labels, epochs = epochs, verbose = 0)\n",
    "            (loss[(l, i, d)], acc[(l, i, d)]) = model.evaluate(val_n, encoded_val_labels, verbose = 0)\n",
    "            print(l, \" layers, \", N, \" nodes, \", d, \" dropout\", acc[(l, i, d)], \"% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing whether more epochs are necessary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will pick the top 5 best architectures from above and check if any of them would benefit from more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5 = [(2, 768, 0.3), (3, 384, 0.2), (3, 384, 0.3), (3, 768, 0.5), (4, 768, 0.5)]\n",
    "acc_e = {}\n",
    "loss_e = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 epochs 2  layers,  768  nodes,  0.3  dropout 0.6714100907104906 % accuracy\n",
      "30 epochs 3  layers,  384  nodes,  0.2  dropout 0.7347994826126839 % accuracy\n",
      "30 epochs 3  layers,  384  nodes,  0.3  dropout 0.7128072445790486 % accuracy\n",
      "30 epochs 3  layers,  768  nodes,  0.5  dropout 0.7089262614737507 % accuracy\n",
      "30 epochs 4  layers,  768  nodes,  0.5  dropout 0.7115135835953549 % accuracy\n",
      "40 epochs 2  layers,  768  nodes,  0.3  dropout 0.7102199225345528 % accuracy\n",
      "40 epochs 3  layers,  384  nodes,  0.2  dropout 0.7153945666620988 % accuracy\n",
      "40 epochs 3  layers,  384  nodes,  0.3  dropout 0.73350582162899 % accuracy\n",
      "40 epochs 3  layers,  768  nodes,  0.5  dropout 0.6908150065454135 % accuracy\n",
      "40 epochs 4  layers,  768  nodes,  0.5  dropout 0.6998706340481362 % accuracy\n",
      "50 epochs 2  layers,  768  nodes,  0.3  dropout 0.7322121605681879 % accuracy\n",
      "50 epochs 3  layers,  384  nodes,  0.2  dropout 0.7542043986018231 % accuracy\n",
      "50 epochs 3  layers,  384  nodes,  0.3  dropout 0.7063389391979301 % accuracy\n",
      "50 epochs 3  layers,  768  nodes,  0.5  dropout 0.7412677879938024 % accuracy\n",
      "50 epochs 4  layers,  768  nodes,  0.5  dropout 0.7244501942033755 % accuracy\n"
     ]
    }
   ],
   "source": [
    "for e in [30, 40, 50]:\n",
    "    for m in top5:\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        (l, N, d) = m\n",
    "        \n",
    "        for n in range(l): \n",
    "            layers.append(tf.keras.layers.Dense(N, activation=tf.nn.relu))\n",
    "            layers.append(tf.keras.layers.Dropout(d))\n",
    "            \n",
    "        layers.append(tf.keras.layers.Dense(n_classes, activation=tf.nn.softmax))\n",
    "            \n",
    "        model = tf.keras.models.Sequential(layers)\n",
    "\n",
    "        model.reset_states()\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "        model.fit(train_n, encoded_train_labels, epochs = e, verbose = 0)\n",
    "        (loss_e[(e, l, i, d)], acc_e[(e, l, i, d)]) = model.evaluate(val_n, encoded_val_labels, verbose = 0)\n",
    "        print(e, \"epochs\", l, \" layers, \", N, \" nodes, \", d, \" dropout\", acc_e[(e, l, i, d)], \"% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varying individual layer widths "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results above, it makes sense to stick to a 3 hidden layer architecture and train it for 50 epochs. Sometimes it helps if the network has layers of different widths. We will test if this is the case in our problem below. We will still vary dropout because it is still unclear whether we should go for a conservative rate of ~0.2 or for the harsher 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "loss_w = {}\n",
    "acc_w = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout: 0.2 layers of widths 192 192 192 => 0.7050452782142362 % accuracy\n",
      "dropout: 0.3 layers of widths 192 192 192 => 0.7011642949932759 % accuracy\n",
      "dropout: 0.5 layers of widths 192 192 192 => 0.7218628720817714 % accuracy\n",
      "dropout: 0.2 layers of widths 192 192 384 => 0.7166882277614549 % accuracy\n",
      "dropout: 0.3 layers of widths 192 192 384 => 0.6804657180589969 % accuracy\n",
      "dropout: 0.5 layers of widths 192 192 384 => 0.6921086676833238 % accuracy\n",
      "dropout: 0.2 layers of widths 192 192 768 => 0.7192755499601673 % accuracy\n",
      "dropout: 0.3 layers of widths 192 192 768 => 0.7011642951089383 % accuracy\n",
      "dropout: 0.5 layers of widths 192 192 768 => 0.7231565331425734 % accuracy\n",
      "dropout: 0.2 layers of widths 192 192 1536 => 0.7231565330269112 % accuracy\n",
      "dropout: 0.3 layers of widths 192 192 1536 => 0.6921086676833238 % accuracy\n",
      "dropout: 0.5 layers of widths 192 192 1536 => 0.7153945666620988 % accuracy\n",
      "dropout: 0.2 layers of widths 192 384 192 => 0.717981888822257 % accuracy\n",
      "dropout: 0.3 layers of widths 192 384 192 => 0.7218628719661091 % accuracy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-664f01f26fb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                     metrics=['accuracy'])\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_train_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mloss_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_val_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"layers of widths\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"=>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"% accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2945\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`inputs` should be a list or tuple.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2947\u001b[0;31m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2948\u001b[0m     \u001b[0mfeed_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2949\u001b[0m     \u001b[0marray_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    467\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    722\u001b[0m   \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m   \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m       \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/_weakrefset.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_IterationGuard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mitemref\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitemref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i1 in [4, 8, 16, 32]:\n",
    "    for i2 in [4, 8, 16, 32]:\n",
    "        for i3 in [4, 8, 16, 32]:\n",
    "            for d in [0.2, 0.3, 0.5]:\n",
    "            \n",
    "                model = tf.keras.models.Sequential([\n",
    "                  tf.keras.layers.Dense(i1 * H, activation=tf.nn.relu),\n",
    "                  tf.keras.layers.Dropout(d),\n",
    "                  tf.keras.layers.Dense(i2 * H, activation=tf.nn.relu),\n",
    "                  tf.keras.layers.Dropout(d),\n",
    "                  tf.keras.layers.Dense(i3 * H, activation=tf.nn.relu),\n",
    "                  tf.keras.layers.Dropout(d),\n",
    "                  tf.keras.layers.Dense(n_classes, activation=tf.nn.softmax)\n",
    "                ])\n",
    "\n",
    "                model.reset_states()\n",
    "\n",
    "                model.compile(optimizer='adam',\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "                model.fit(train_n, encoded_train_labels, epochs = epochs, verbose = 0)\n",
    "                (loss_w[(d, i1, i2, i3)], acc_w[(d, i1, i2, i3)]) = model.evaluate(val_n, encoded_val_labels, verbose = 0)\n",
    "                print(\"dropout:\", d, \"layers of widths\", i1 * H, i2 * H, i3 * H, \"=>\", acc_w[(d, i1, i2, i3)], \"% accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout: 0.2 layers of widths 192 384 384 => 0.6985769729873341 % accuracy\n",
      "dropout: 0.3 layers of widths 192 384 384 => 0.7244501940877133 % accuracy\n",
      "dropout: 0.5 layers of widths 192 384 384 => 0.7076326004129486 % accuracy\n",
      "dropout: 0.2 layers of widths 192 384 768 => 0.7205692109438612 % accuracy\n",
      "dropout: 0.3 layers of widths 192 384 768 => 0.7360931436349318 % accuracy\n",
      "dropout: 0.5 layers of widths 192 384 768 => 0.7102199223996134 % accuracy\n",
      "dropout: 0.2 layers of widths 192 384 1536 => 0.7179818888993652 % accuracy\n"
     ]
    }
   ],
   "source": [
    "for i1 in [4, 8, 16, 32]:\n",
    "    for i2 in [8, 16, 32]:\n",
    "        for i3 in [4, 8, 16, 32]:\n",
    "            for d in [0.2, 0.3, 0.5]:\n",
    "                \n",
    "                if(i3 == 4 and d != 0.5):\n",
    "                    break\n",
    "            \n",
    "                model = tf.keras.models.Sequential([\n",
    "                  tf.keras.layers.Dense(i1 * H, activation=tf.nn.relu),\n",
    "                  tf.keras.layers.Dropout(d),\n",
    "                  tf.keras.layers.Dense(i2 * H, activation=tf.nn.relu),\n",
    "                  tf.keras.layers.Dropout(d),\n",
    "                  tf.keras.layers.Dense(i3 * H, activation=tf.nn.relu),\n",
    "                  tf.keras.layers.Dropout(d),\n",
    "                  tf.keras.layers.Dense(n_classes, activation=tf.nn.softmax)\n",
    "                ])\n",
    "\n",
    "                model.reset_states()\n",
    "\n",
    "                model.compile(optimizer='adam',\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "                model.fit(train_n, encoded_train_labels, epochs = epochs, verbose = 0)\n",
    "                (loss_w[(d, i1, i2, i3)], acc_w[(d, i1, i2, i3)]) = model.evaluate(val_n, encoded_val_labels, verbose = 0)\n",
    "                print(\"dropout:\", d, \"layers of widths\", i1 * H, i2 * H, i3 * H, \"=>\", acc_w[(d, i1, i2, i3)], \"% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect which architectures did better than 72% accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k:v for (k,v) in acc_w.items() if v > 0.72}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(N1, N2, N3) = (4 * H, 8 * H, 8 * H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization may synergize with dropout so we vary d when inspecting the effectiveness of batch normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in [0.2, 0.3, 0.5]:\n",
    "    model = tf.keras.models.Sequential([\n",
    "                  tf.keras.layers.Dense(N1, activation=tf.nn.relu),\n",
    "                  tf.keras.layers.Dropout(d),\n",
    "                  tf.keras.layers.BatchNormalization(),\n",
    "                  tf.keras.layers.Dense(N2, activation=tf.nn.relu),\n",
    "                  tf.keras.layers.Dropout(d),\n",
    "                  tf.keras.layers.BatchNormalization(),\n",
    "                  tf.keras.layers.Dense(N3, activation=tf.nn.relu),\n",
    "                  tf.keras.layers.Dropout(d),\n",
    "                  tf.keras.layers.BatchNormalization(),\n",
    "                  tf.keras.layers.Dense(n_classes, activation=tf.nn.softmax)\n",
    "                ])\n",
    "\n",
    "    model.reset_states()\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_n, encoded_train_labels, epochs = epochs)\n",
    "    print(\"Dropout: \", d)\n",
    "    model.evaluate(val_n, encoded_val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring various activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_f = np.zeros(6)\n",
    "loss_f = np.zeros(6)\n",
    "function_name = [\"sigmoid\", \"hard sigmoid\", \"tanh\", \"relu\", \"leaky relu\", \"selu\"]\n",
    "t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu_01(x):\n",
    "    return tf.nn.leaky_relu(x, alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in [tf.nn.sigmoid, tf.nn.hard_sigmoid, tf.nn.tanh, tf.nn.relu, leaky_relu_01, tf.nn.selu]:\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "              tf.keras.layers.Dense(N1, activation=f),\n",
    "              tf.keras.layers.Dropout(dropout_rate),\n",
    "              tf.keras.layers.Dense(N2, activation=f),\n",
    "              tf.keras.layers.Dropout(dropout_rate),\n",
    "              tf.keras.layers.Dense(N3, activation=f),\n",
    "              tf.keras.layers.Dropout(dropout_rate),\n",
    "              tf.keras.layers.Dense(n_classes, activation=tf.nn.softmax)\n",
    "            ])\n",
    "\n",
    "    model.reset_states()\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "          loss='categorical_crossentropy',\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_n, encoded_train_labels, epochs = epochs, verbose = 0)\n",
    "    (loss_f[t], acc_f[t]) = model.evaluate(val_n, encoded_val_labels, verbose = 0)\n",
    "    print(function_name[t] \"=>\", acc_f[t], \"% accuracy\")\n",
    "    t++"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
