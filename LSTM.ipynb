{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas\n",
    "import xarray\n",
    "import cftime\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:         (bounds: 2, month: 12, time: 7800)\n",
      "Coordinates:\n",
      "  * time            (time) float64 15.5 45.0 74.5 ... 2.372e+05 2.372e+05\n",
      "Dimensions without coordinates: bounds, month\n",
      "Data variables:\n",
      "    nino34          (time) float64 ...\n",
      "    time_bnds       (time, bounds) float64 ...\n",
      "    areacello       float32 ...\n",
      "    days_per_month  (month) int32 ...\n"
     ]
    }
   ],
   "source": [
    "datapath = 'nino34_monthly.nc'\n",
    "nino34 = xarray.open_dataset(datapath, decode_times = False)\n",
    "print(nino34)\n",
    "nino34 = np.array(nino34['nino34'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ONI(nino34, m = 3):\n",
    "    oni = np.array(nino34)\n",
    "    length = nino34.shape[0]\n",
    "    for i in range(length):\n",
    "        oni[i] = np.mean(nino34[max(0, (i - m + 1)) : min((i + 1), length)])\n",
    "    return oni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "oni = ONI(nino34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def climatology(nino34):\n",
    "    clm = np.zeros(12)\n",
    "    length = nino34.shape[0]\n",
    "    for month in range(12):\n",
    "        section = [12 * i + month for i in range(length // 12)]\n",
    "        clm[month] = np.mean(nino34[section])\n",
    "    return clm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "clm = climatology(nino34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SST_anomaly(nino34, clm):\n",
    "    anm = np.array(nino34)\n",
    "    length = nino34.shape[0]\n",
    "    for i in range(length):\n",
    "        anm[i] = nino34[i] - clm[i % 12]\n",
    "    return anm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "anm = SST_anomaly(nino34, clm)\n",
    "oanm = ONI(anm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 7      # prediction timeline\n",
    "H = 32   # history used for prediction\n",
    "include_month = 1           # 1 if we use the month as a feature, 0 otherwise\n",
    "threshold = 0.5         \n",
    "signal = np.array(nino34[12 * 50:])   # data used for training/testing\n",
    "length = signal.shape[0]    # number of data points\n",
    "\n",
    "mean = np.mean(signal)\n",
    "std = np.std(signal)\n",
    "\n",
    "signal = (signal - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 7000\n",
    "sample_len = 350\n",
    "n_samples = size // sample_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "        \n",
    "# create the 'history matrix'\n",
    "data = np.ndarray((n_samples, sample_len, H + include_month))\n",
    "for i in range(n_samples):\n",
    "    for j in range(sample_len):\n",
    "        t = sample_len * i + j\n",
    "        if(include_month == False):\n",
    "             data[i,j] = signal[t:(t + H)]\n",
    "        else:\n",
    "            data[i,j] = np.append(signal[t:(t + H)], (t + H + T) % 12 / 12)\n",
    "\n",
    "labels = np.ndarray((n_samples, sample_len), dtype = int)\n",
    "        \n",
    "for i in range(n_samples):\n",
    "    for j in range(sample_len):\n",
    "        t = sample_len * i + j\n",
    "        labels[i, j] = oanm[t + H + T]\n",
    "\n",
    "split = n_samples // 10      \n",
    "shuffle = np.random.permutation(n_samples)\n",
    "train_ind = np.array(shuffle[0: 8 * split])\n",
    "val_ind = np.array(shuffle[8 * split: 9 * split])\n",
    "test_ind = np.array(shuffle[9 * split: size])\n",
    "\n",
    "train = np.array(data[train_ind])\n",
    "train_labels = np.array(labels[train_ind])\n",
    "\n",
    "val = np.array(data[val_ind])\n",
    "val_labels = np.array(labels[val_ind])\n",
    "\n",
    "test = np.array(data[test_ind])\n",
    "test_labels = np.array(labels[test_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 mse : 0.4439932703971863 0.4565284252166748\n",
      "10 mae : 0.28593870997428894 0.3852995038032532\n",
      "20 mse : 0.2769436836242676 0.3629114627838135\n",
      "20 mae : 0.2807210683822632 0.3219720125198364\n",
      "30 mse : 0.34416961669921875 0.38427823781967163\n",
      "30 mae : 0.32868683338165283 0.31550246477127075\n",
      "40 mse : 0.3221272826194763 0.36283326148986816\n",
      "40 mae : 0.27166908979415894 0.3132445216178894\n",
      "50 mse : 0.32102036476135254 0.30800414085388184\n",
      "50 mae : 0.357069194316864 0.3703349828720093\n",
      "60 mse : 0.3003605604171753 0.34306246042251587\n",
      "60 mae : 0.3151930570602417 0.2767164707183838\n",
      "70 mse : 0.271484375 0.32294368743896484\n",
      "70 mae : 0.28286606073379517 0.31111547350883484\n",
      "80 mse : 0.30231577157974243 0.31452441215515137\n",
      "80 mae : 0.3346917927265167 0.3304855227470398\n",
      "90 mse : 0.34859979152679443 0.3579505681991577\n",
      "90 mae : 0.3212650418281555 0.287781298160553\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "        \n",
    "acc = 0\n",
    "loss = 0\n",
    "\n",
    "\n",
    "for n_neurons in range(10, 100, 10):\n",
    "    for l in ['mse', 'mae']:\n",
    "\n",
    "        model = tf.keras.models.Sequential([\n",
    "                  tf.keras.layers.LSTM(n_neurons),\n",
    "                  tf.keras.layers.Dense(1)\n",
    "                ])\n",
    "\n",
    "        model.reset_states()\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                loss = l,\n",
    "                metrics=['mse', 'mae'])\n",
    "\n",
    "        model.fit(train, train_labels, batch_size=n_samples, epochs = epochs, verbose = 0)\n",
    "        (loss, mse, mae) = model.evaluate(val, val_labels, verbose = 0)\n",
    "\n",
    "        print(n_neurons, l, \":\", mse, mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Different Data Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(size, sample_len):\n",
    "    n_samples = size // sample_len\n",
    "    np.random.seed(0)\n",
    "        \n",
    "    # create the 'history matrix'\n",
    "    data = np.ndarray((n_samples, sample_len, H + include_month))\n",
    "    for i in range(n_samples):\n",
    "        for j in range(sample_len):\n",
    "            t = sample_len * i + j\n",
    "            if(include_month == False):\n",
    "                 data[i,j] = signal[t:(t + H)]\n",
    "            else:\n",
    "                data[i,j] = np.append(signal[t:(t + H)], (t + H + T) % 12 / 12)\n",
    "\n",
    "    labels = np.ndarray((n_samples, sample_len), dtype = int)\n",
    "        \n",
    "    for i in range(n_samples):\n",
    "        for j in range(sample_len):\n",
    "            t = sample_len * i + j\n",
    "            labels[i, j] = oanm[t + H + T]\n",
    "\n",
    "    split = n_samples // 10      \n",
    "    shuffle = np.random.permutation(n_samples)\n",
    "    train_ind = np.array(shuffle[0: 8 * split])\n",
    "    val_ind = np.array(shuffle[8 * split: 9 * split])\n",
    "    test_ind = np.array(shuffle[9 * split: size])\n",
    "\n",
    "    train = np.array(data[train_ind])\n",
    "    train_labels = np.array(labels[train_ind])\n",
    "\n",
    "    val = np.array(data[val_ind])\n",
    "    val_labels = np.array(labels[val_ind])\n",
    "\n",
    "    test = np.array(data[test_ind])\n",
    "    test_labels = np.array(labels[test_ind])\n",
    "    \n",
    "    return (train, train_labels, val, val_labels, test, test_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175 mae : 0.20791709423065186 0.23351305723190308\n",
      "175 mse : 0.2148858904838562 0.25946658849716187\n",
      "350 mae : 0.36433857679367065 0.35473954677581787\n",
      "350 mse : 0.34228917956352234 0.3974469304084778\n",
      "525 mae : 0.3219010829925537 0.4519275724887848\n",
      "525 mse : 0.23203979432582855 0.2688826322555542\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "n_neurons = 40\n",
    "\n",
    "for sample_len in range(175, 700, 175):\n",
    "    for l in ['mae', 'mse']:\n",
    "        (train, train_labels, val, val_labels, test, test_labels) = generate_data(7000, sample_len)\n",
    "        \n",
    "        n_samples = 7000 // sample_len\n",
    "        \n",
    "        model = tf.keras.models.Sequential([\n",
    "             tf.keras.layers.LSTM(n_neurons),\n",
    "             tf.keras.layers.Dense(1)\n",
    "                ])\n",
    "\n",
    "        model.reset_states()\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                loss = l,\n",
    "                metrics=['mse', 'mae'])\n",
    "\n",
    "        model.fit(train, train_labels, batch_size=n_samples, epochs = epochs, verbose = 0)\n",
    "        (loss, mse, mae) = model.evaluate(val, val_labels, verbose = 0)\n",
    "\n",
    "        print(sample_len, l, \":\", mse, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 mae : 0.1931416541337967 0.1976475715637207\n",
      "25 mse : 0.22675611078739166 0.27281227707862854\n",
      "75 mae : 0.2589319944381714 0.2879652678966522\n",
      "75 mse : 0.23260360956192017 0.29476746916770935\n",
      "125 mae : 0.4088299870491028 0.3584330677986145\n",
      "125 mse : 0.4196997284889221 0.3758447468280792\n",
      "175 mae : 0.20199579000473022 0.2518816888332367\n",
      "175 mse : 0.20040751993656158 0.21377389132976532\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "n_neurons = 40\n",
    "\n",
    "for sample_len in range(25, 176, 50):\n",
    "    for l in ['mae', 'mse']:\n",
    "        (train, train_labels, val, val_labels, test, test_labels) = generate_data(7000, sample_len)\n",
    "        \n",
    "        n_samples = 7000 // sample_len\n",
    "        \n",
    "        model = tf.keras.models.Sequential([\n",
    "             tf.keras.layers.LSTM(n_neurons),\n",
    "             tf.keras.layers.Dense(1)\n",
    "                ])\n",
    "\n",
    "        model.reset_states()\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                loss = l,\n",
    "                metrics=['mse', 'mae'])\n",
    "\n",
    "        model.fit(train, train_labels, batch_size=n_samples, epochs = epochs, verbose = 0)\n",
    "        (loss, mse, mae) = model.evaluate(val, val_labels, verbose = 0)\n",
    "\n",
    "        print(sample_len, l, \":\", mse, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 mae : 0.2557797520501273 0.2254977747797966\n",
      "1 mse : 0.3479794868401119 0.42146564160074507\n",
      "2 mae : 0.22148669157709394 0.1954586889062609\n",
      "2 mse : 0.4869651378052575 0.4785437212671552\n",
      "3 mae : 0.3171760131616961 0.26406358128069807\n",
      "3 mse : 0.6210058873815086 0.5460952113626341\n",
      "4 mae : 0.2620085006100791 0.21815091337476458\n",
      "4 mse : 0.5294170721939632 0.5154748010635376\n",
      "5 mae : 0.3209615869181497 0.2606730282306671\n",
      "5 mse : 0.6040602488177164 0.5626546110425676\n",
      "6 mae : 0.2950953882316063 0.25973642077939263\n",
      "6 mse : 0.6084838201259745 0.5727957857066187\n",
      "7 mae : 0.21050041675567627 0.22100984811782837\n",
      "7 mse : 0.365741171836853 0.4507310485839844\n",
      "8 mae : 0.2969372246799798 0.26481383507964257\n",
      "8 mse : 0.4747670741601922 0.5010209090408237\n",
      "9 mae : 0.2874623378375908 0.2657031209824921\n",
      "9 mse : 0.4010944796072972 0.4619648940377421\n",
      "10 mae : 0.31363159332956586 0.30367177384240285\n",
      "10 mse : 0.4688867798873356 0.5035372691495078\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "for sample_len in range(1, 11):\n",
    "    for l in ['mae', 'mse']:\n",
    "        (train, train_labels, val, val_labels, test, test_labels) = generate_data(7000, sample_len)\n",
    "        \n",
    "        n_samples = 7000 // sample_len\n",
    "        \n",
    "        model = tf.keras.models.Sequential([\n",
    "             tf.keras.layers.LSTM(n_neurons),\n",
    "             tf.keras.layers.Dense(1)\n",
    "                ])\n",
    "\n",
    "        model.reset_states()\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                loss = l,\n",
    "                metrics=['mse', 'mae'])\n",
    "\n",
    "        model.fit(train, train_labels, batch_size=n_samples, epochs = epochs, verbose = 0)\n",
    "        (loss, mse, mae) = model.evaluate(val, val_labels, verbose = 0)\n",
    "\n",
    "        print(sample_len, l, \":\", mse, mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Multiple Fully Connected Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 0.19299684464931488 0.18327488005161285\n",
      "64 0.19112162292003632 0.17845840752124786\n",
      "96 0.19061000645160675 0.17806892096996307\n",
      "128 0.19148209691047668 0.17133986949920654\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "n_neurons = 40\n",
    "sample_len = 25\n",
    "n_samples = 7000 // sample_len\n",
    "(train, train_labels, val, val_labels, test, test_labels) = generate_data(7000, sample_len)\n",
    "\n",
    "for N in [H, 2 * H, 3 * H, 4 * H]:\n",
    "    model = tf.keras.models.Sequential([\n",
    "             tf.keras.layers.LSTM(n_neurons),\n",
    "             tf.keras.layers.Dense(N),\n",
    "             tf.keras.layers.Dense(1)\n",
    "            ])\n",
    "\n",
    "    model.reset_states()\n",
    "\n",
    "    model.compile(optimizer='adam', loss = 'mae', metrics=['mse', 'mae'])\n",
    "\n",
    "    model.fit(train, train_labels, batch_size=n_samples, epochs = epochs, verbose = 0)\n",
    "    (loss, mse, mae) = model.evaluate(val, val_labels, verbose = 0)\n",
    "\n",
    "    print(N, mse, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 96 0.1914338320493698 0.17169751226902008\n",
      "96 128 0.19182442128658295 0.16956207156181335\n",
      "96 160 0.19161559641361237 0.17066515982151031\n",
      "96 192 0.19169606268405914 0.17110373079776764\n",
      "128 96 0.19167205691337585 0.17395862936973572\n",
      "128 128 0.1915774643421173 0.1689101606607437\n",
      "128 160 0.19137586653232574 0.1709912121295929\n",
      "128 192 0.19163250923156738 0.17017783224582672\n",
      "160 96 0.19150756299495697 0.1695082187652588\n",
      "160 128 0.1913897842168808 0.16959922015666962\n",
      "160 160 0.19144029915332794 0.16979756951332092\n",
      "160 192 0.19227100908756256 0.17498715221881866\n",
      "192 96 0.19215954840183258 0.1737329065799713\n",
      "192 128 0.19164560735225677 0.16979564726352692\n",
      "192 160 0.19223950803279877 0.17494085431098938\n",
      "192 192 0.19122233986854553 0.17242108285427094\n"
     ]
    }
   ],
   "source": [
    "# MAE\n",
    "for N1 in [3 * H, 4 * H, 5 * H, 6 * H]:\n",
    "    for N2 in [3 * H, 4 * H, 5 * H, 6 * H]:\n",
    "        model = tf.keras.models.Sequential([\n",
    "             tf.keras.layers.LSTM(n_neurons),\n",
    "             tf.keras.layers.Dense(N1),\n",
    "             tf.keras.layers.Dense(N2),\n",
    "             tf.keras.layers.Dense(1)\n",
    "            ])\n",
    "\n",
    "        model.reset_states()\n",
    "\n",
    "        model.compile(optimizer='adam', loss = 'mae', metrics=['mse', 'mae'])\n",
    "    \n",
    "        model.fit(train, train_labels, batch_size=n_samples, epochs = epochs, verbose = 0)\n",
    "        (loss, mse, mae) = model.evaluate(val, val_labels, verbose = 0)\n",
    "\n",
    "        print(N1, N2, mse, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 96 0.24344365298748016 0.31899362802505493\n",
      "96 128 0.27740979194641113 0.3390839397907257\n",
      "96 160 0.23378729820251465 0.28796225786209106\n",
      "96 192 0.2612089216709137 0.3246050775051117\n",
      "128 96 0.2736087739467621 0.3388022780418396\n",
      "128 128 0.22609065473079681 0.2659027576446533\n",
      "128 160 0.2801613211631775 0.34527701139450073\n",
      "128 192 0.26062971353530884 0.31787484884262085\n",
      "160 96 0.2606222331523895 0.34885692596435547\n",
      "160 128 0.3266465961933136 0.39134034514427185\n",
      "160 160 0.3009677827358246 0.3586064279079437\n",
      "160 192 0.24563777446746826 0.306509405374527\n",
      "192 96 0.34183770418167114 0.3709569275379181\n",
      "192 128 0.26682230830192566 0.3506458103656769\n",
      "192 160 0.2602820098400116 0.3616555333137512\n",
      "192 192 0.2504517138004303 0.3343135714530945\n"
     ]
    }
   ],
   "source": [
    "# MSE\n",
    "for N1 in [3 * H, 4 * H, 5 * H, 6 * H]:\n",
    "    for N2 in [3 * H, 4 * H, 5 * H, 6 * H]:\n",
    "        model = tf.keras.models.Sequential([\n",
    "             tf.keras.layers.LSTM(n_neurons),\n",
    "             tf.keras.layers.Dense(N1),\n",
    "             tf.keras.layers.Dense(N2),\n",
    "             tf.keras.layers.Dense(1)\n",
    "            ])\n",
    "\n",
    "        model.reset_states()\n",
    "\n",
    "        model.compile(optimizer='adam', loss = 'mse', metrics=['mse', 'mae'])\n",
    "    \n",
    "        model.fit(train, train_labels, batch_size=n_samples, epochs = epochs, verbose = 0)\n",
    "        (loss, mse, mae) = model.evaluate(val, val_labels, verbose = 0)\n",
    "\n",
    "        print(N1, N2, mse, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
